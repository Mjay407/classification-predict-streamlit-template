{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Libraries for data loading, data manipulation and data visualisation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>message</th>\n",
       "      <th>tweetid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>RT @darreljorstad: Funny as hell! Canada deman...</td>\n",
       "      <td>897853122080407553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1</td>\n",
       "      <td>All the biggest lies about climate change and ...</td>\n",
       "      <td>925046776553529344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1</td>\n",
       "      <td>The Coming Revelation Of The $q$Global Warming...</td>\n",
       "      <td>696354236850786305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1</td>\n",
       "      <td>RT @DineshDSouza: Let's see if the world ends ...</td>\n",
       "      <td>846806509732483072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1</td>\n",
       "      <td>RT @SteveSGoddard: Obama has no control over t...</td>\n",
       "      <td>628085266293653504</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment                                            message  \\\n",
       "0         -1  RT @darreljorstad: Funny as hell! Canada deman...   \n",
       "1         -1  All the biggest lies about climate change and ...   \n",
       "2         -1  The Coming Revelation Of The $q$Global Warming...   \n",
       "3         -1  RT @DineshDSouza: Let's see if the world ends ...   \n",
       "4         -1  RT @SteveSGoddard: Obama has no control over t...   \n",
       "\n",
       "              tweetid  \n",
       "0  897853122080407553  \n",
       "1  925046776553529344  \n",
       "2  696354236850786305  \n",
       "3  846806509732483072  \n",
       "4  628085266293653504  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = pd.read_csv('train.csv') # Read in csv file as pandas dataframe\n",
    "df2 = pd.read_csv('train copy.csv') # Read in csv file as pandas dataframe\n",
    "\n",
    "df_train = pd.concat([df1, df2], ignore_index=True) # Concatenate the two dataframes\n",
    "df_train.head() # Check the first few rows of the dataframe\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern_url = [r'http[s]?://(?:[A-Za-z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9A-Fa-f][0-9A-Fa-f]))+', r'@\\w+\\b', r'RT', r'\\#\\w+\\b', r\"\\b(?:climate change|global warming)\\b\"]\n",
    "\n",
    "df_train['message'] = df_train['message'].replace(to_replace = pattern_url, value = '', regex = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_non_english_words(text):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    # Regular expression pattern to match English words and apostrophes\n",
    "    pattern = re.compile(r\"\\b[a-zA-Z']+\")\n",
    "\n",
    "    # Find all English words and apostrophes in the text\n",
    "    english_words = re.findall(pattern, text)\n",
    "\n",
    "    # Join the English words into a single string\n",
    "    result = ' '.join(english_words)\n",
    "\n",
    "    return result\n",
    "\n",
    "df_train['message'] = df_train['message'].apply(remove_non_english_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['message'] = df_train['message'].str.lower()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_https(text):\n",
    "    return re.sub(r'https', ' ', text)\n",
    "\n",
    "df_train['message'] = df_train['message'].apply(remove_https)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_train = df_train.drop_duplicates(keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lem = WordNetLemmatizer()\n",
    "\n",
    "def text_processing(text):\n",
    "    text = nltk.word_tokenize(text.lower())\n",
    "    y = []\n",
    "    for i in text:\n",
    "        if i.isalnum():\n",
    "            y.append(i)\n",
    "\n",
    "    text = y[:]\n",
    "    y.clear()\n",
    "    \n",
    "    for i in text:\n",
    "        if i not in stopwords.words('english') and i not in string.punctuation:\n",
    "            lemming = lem.lemmatize(i)\n",
    "            y.append(lemming)\n",
    "            \n",
    "    return \" \".join(y)\n",
    "\n",
    "df_train['message_transformed'] = df_train['message'].apply(text_processing)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_train['message_transformed']\n",
    "y = df_train['sentiment']\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "X_train_vec = vectorizer.fit_transform(X_train)\n",
    "X_test_vec = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.83      0.49      0.62       798\n",
      "           0       0.74      0.53      0.62      1569\n",
      "           1       0.77      0.92      0.84      4927\n",
      "           2       0.82      0.75      0.78      2022\n",
      "\n",
      "    accuracy                           0.78      9316\n",
      "   macro avg       0.79      0.67      0.72      9316\n",
      "weighted avg       0.78      0.78      0.77      9316\n",
      "\n"
     ]
    }
   ],
   "source": [
    "svm = SVC()\n",
    "svm.fit(X_train_vec, y_train)\n",
    "\n",
    "svm_y_pred = svm.predict(X_test_vec)\n",
    "print(classification_report(y_test, svm_y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('svm.pkl', 'wb') as file:\n",
    "    pickle.dump(svm, file)\n",
    "\n",
    "with open('vectorizer.pkl', 'wb') as file:\n",
    "    pickle.dump(vectorizer, file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
